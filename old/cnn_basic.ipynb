{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.1.0\n",
      "Torchvision Version:  0.2.2\n",
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/train/spectograms\"\n",
    "\n",
    "model_name = \"custom\"\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "feature_extract = False # only update the reshaped layer params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # input.size: 3x224x224\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            # output: 16x224x224\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # output: 16x112x112\n",
    "            \n",
    "            \n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            # output: 128x112x112\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            #output: 32x56x56\n",
    "            \n",
    "            \n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            # output: 64x56x56\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            # output: 64x56x56\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),            \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            #output: 64x28x28\n",
    "\n",
    "            \n",
    "            \n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            # output: 64x14x14\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),            \n",
    "            nn.AdaptiveAvgPool2d((1,1))\n",
    "            # output: 256x7x7\n",
    "            \n",
    "            \n",
    "            \n",
    "        )\n",
    "        self.linear_layer = nn.Sequential(\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = self.conv_layers(input)\n",
    "        output = output.view(input.size(0), -1)\n",
    "        output = self.linear_layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
    "    since = time.time()\n",
    "    \n",
    "    val_acc_history = []\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-'*10)\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "            \n",
    "        print()\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "    \n",
    "    if model_name == \"squeezenet\":\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "        \n",
    "    if model_name == \"resnet\":\n",
    "        model_ft = models.resnet34(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "        \n",
    "    if model_name == \"custom\":\n",
    "        model_ft = ConvNet()\n",
    "        input_size = 224\n",
    "        \n",
    "    return model_ft, input_size\n",
    "\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(input_size),\n",
    "    transforms.CenterCrop(input_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "image_dataset = datasets.ImageFolder(data_dir, data_transforms)\n",
    "\n",
    "validation_split = 0.2\n",
    "random_seed = 42\n",
    "\n",
    "dataset_size = len(image_dataset)\n",
    "split = int(validation_split * dataset_size)\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "indices = np.random.permutation(dataset_size)\n",
    "\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = dataset_size - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(image_dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "dataloaders_dict = {}\n",
    "dataloaders_dict['train'] = train_dataloader\n",
    "dataloaders_dict['val'] = val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.25"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t conv_layers.0.weight\n",
      "\t conv_layers.0.bias\n",
      "\t conv_layers.2.weight\n",
      "\t conv_layers.2.bias\n",
      "\t conv_layers.4.weight\n",
      "\t conv_layers.4.bias\n",
      "\t conv_layers.6.weight\n",
      "\t conv_layers.6.bias\n",
      "\t conv_layers.8.weight\n",
      "\t conv_layers.8.bias\n",
      "\t conv_layers.10.weight\n",
      "\t conv_layers.10.bias\n",
      "\t conv_layers.11.weight\n",
      "\t conv_layers.11.bias\n",
      "\t conv_layers.13.weight\n",
      "\t conv_layers.13.bias\n",
      "\t conv_layers.15.weight\n",
      "\t conv_layers.15.bias\n",
      "\t conv_layers.17.weight\n",
      "\t conv_layers.17.bias\n",
      "\t linear_layer.0.weight\n",
      "\t linear_layer.0.bias\n"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 1.8167 Acc: 0.3540\n",
      "val Loss: 1.7845 Acc: 0.3339\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 1.3813 Acc: 0.5168\n",
      "val Loss: 1.3709 Acc: 0.5023\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 1.1472 Acc: 0.6083\n",
      "val Loss: 1.1516 Acc: 0.5805\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.9792 Acc: 0.6711\n",
      "val Loss: 0.9524 Acc: 0.6329\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.8289 Acc: 0.7298\n",
      "val Loss: 1.1186 Acc: 0.6035\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.7262 Acc: 0.7659\n",
      "val Loss: 0.8842 Acc: 0.6937\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 0.6394 Acc: 0.7974\n",
      "val Loss: 0.6640 Acc: 0.7700\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 0.5630 Acc: 0.8238\n",
      "val Loss: 0.5644 Acc: 0.8188\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 0.5056 Acc: 0.8461\n",
      "val Loss: 0.7883 Acc: 0.7203\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 0.4224 Acc: 0.8742\n",
      "val Loss: 0.6430 Acc: 0.7663\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 0.3987 Acc: 0.8765\n",
      "val Loss: 0.3886 Acc: 0.8767\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.3523 Acc: 0.8935\n",
      "val Loss: 0.4227 Acc: 0.8657\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.3194 Acc: 0.9029\n",
      "val Loss: 0.4348 Acc: 0.8565\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.3008 Acc: 0.9062\n",
      "val Loss: 0.3853 Acc: 0.8721\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.2523 Acc: 0.9253\n",
      "val Loss: 0.3838 Acc: 0.8712\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.2442 Acc: 0.9241\n",
      "val Loss: 0.4821 Acc: 0.8427\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.2335 Acc: 0.9280\n",
      "val Loss: 0.3134 Acc: 0.8951\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.2051 Acc: 0.9402\n",
      "val Loss: 0.2648 Acc: 0.9172\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.1618 Acc: 0.9526\n",
      "val Loss: 0.2840 Acc: 0.9043\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.1497 Acc: 0.9563\n",
      "val Loss: 0.2816 Acc: 0.9016\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 0.1572 Acc: 0.9510\n",
      "val Loss: 0.2793 Acc: 0.9080\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.1267 Acc: 0.9650\n",
      "val Loss: 0.3435 Acc: 0.8822\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.1371 Acc: 0.9598\n",
      "val Loss: 0.2655 Acc: 0.9200\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.1045 Acc: 0.9726\n",
      "val Loss: 0.1759 Acc: 0.9384\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.0986 Acc: 0.9731\n",
      "val Loss: 0.2586 Acc: 0.9052\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.0897 Acc: 0.9795\n",
      "val Loss: 0.2056 Acc: 0.9374\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.0785 Acc: 0.9798\n",
      "val Loss: 0.1678 Acc: 0.9466\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.0700 Acc: 0.9832\n",
      "val Loss: 0.2103 Acc: 0.9328\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.0831 Acc: 0.9816\n",
      "val Loss: 0.2848 Acc: 0.9126\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.0736 Acc: 0.9788\n",
      "val Loss: 0.3753 Acc: 0.8749\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 0.0669 Acc: 0.9828\n",
      "val Loss: 0.2134 Acc: 0.9246\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.0588 Acc: 0.9864\n",
      "val Loss: 0.1600 Acc: 0.9411\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.0417 Acc: 0.9894\n",
      "val Loss: 0.1706 Acc: 0.9448\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.0463 Acc: 0.9885\n",
      "val Loss: 0.1705 Acc: 0.9430\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.0345 Acc: 0.9929\n",
      "val Loss: 0.3112 Acc: 0.9154\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.0479 Acc: 0.9876\n",
      "val Loss: 0.1539 Acc: 0.9466\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.0443 Acc: 0.9908\n",
      "val Loss: 0.1263 Acc: 0.9604\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.0230 Acc: 0.9970\n",
      "val Loss: 0.1445 Acc: 0.9522\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.0194 Acc: 0.9970\n",
      "val Loss: 0.1217 Acc: 0.9641\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.0192 Acc: 0.9977\n",
      "val Loss: 0.1552 Acc: 0.9512\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 0.0183 Acc: 0.9968\n",
      "val Loss: 0.1372 Acc: 0.9568\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.0268 Acc: 0.9933\n",
      "val Loss: 0.1493 Acc: 0.9604\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.0357 Acc: 0.9913\n",
      "val Loss: 0.1953 Acc: 0.9374\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.0319 Acc: 0.9929\n",
      "val Loss: 0.1678 Acc: 0.9512\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.0233 Acc: 0.9949\n",
      "val Loss: 0.2374 Acc: 0.9301\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.0246 Acc: 0.9940\n",
      "val Loss: 0.1277 Acc: 0.9595\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.0262 Acc: 0.9947\n",
      "val Loss: 0.2726 Acc: 0.9246\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.0244 Acc: 0.9936\n",
      "val Loss: 0.1610 Acc: 0.9522\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.0198 Acc: 0.9959\n",
      "val Loss: 0.1322 Acc: 0.9632\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.0111 Acc: 0.9979\n",
      "val Loss: 0.1381 Acc: 0.9632\n",
      "\n",
      "Training complete in 6m 32s\n",
      "Best val Acc: 0.964121\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filepath = './saved_models/MFCC_CNN_96'\n",
    "\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model_ft.state_dict(),\n",
    "    'optimizer_state_dict': optimizer_ft.state_dict(),\n",
    "}, model_filepath)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_venv",
   "language": "python",
   "name": "cuda_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
