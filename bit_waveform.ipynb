{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import adabound\n",
    "\n",
    "from trainer_helper import Trainer_Helper\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "random.seed(23)\n",
    "torch.manual_seed(23)\n",
    "if device == \"cuda:0\":\n",
    "    torch.cuda.manual_seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"UrbanSound8K/bitmap/\"\n",
    "num_classes = 10\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitmap_loader(path):\n",
    "    with np.load(path) as data:\n",
    "        #print(data['arr_0'].shape)\n",
    "        data_len = data['arr_0'].shape[1]\n",
    "        arr = data['arr_0']\n",
    "        if data_len > 22050:\n",
    "            data_len = 22050\n",
    "            arr = arr[:,0:22050]\n",
    "        assert arr.shape[1] <= 22050\n",
    "        #try:\n",
    "        arr = np.pad(arr, ((0, 0), (0, 22050-data_len)), 'constant')\n",
    "        #except:\n",
    "        #    print(data['arr_0'].shape)\n",
    "        #    print(arr.shape)\n",
    "        #arr = data['arr_0']\n",
    "        result = []\n",
    "        for row in arr:\n",
    "            unpacked_row = np.unpackbits(row)\n",
    "            result.append(unpacked_row)\n",
    "\n",
    "        #return torch.FloatTensor(result)\n",
    "        return np.array(result)\n",
    "\n",
    "\n",
    "train_dataset = datasets.DatasetFolder(data_dir + 'train/', loader=bitmap_loader, extensions='npz')\n",
    "val_dataset = datasets.DatasetFolder(data_dir + 'val/', loader=bitmap_loader, extensions='npz')\n",
    "test_dataset = datasets.DatasetFolder(data_dir + 'test/', loader=bitmap_loader, extensions='npz')\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, pin_memory =True, shuffle=True, num_workers=4)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, pin_memory=True, num_workers=4)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, hidden_size=256, lstm_layers=2, cnn_start_channels=256):\n",
    "        super(RNN, self).__init__()\n",
    "        self.name = \"CNN({})_LSTM({}_hidden_{})\".format(cnn_start_channels, lstm_layers, cnn_start_channels)\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # input.size: 16x176400\n",
    "            nn.Conv1d(in_channels=16, out_channels=cnn_start_channels, kernel_size=30, stride=10),\n",
    "            # output: 64 x 17638\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(cnn_start_channels),\n",
    "            # output 64x17638\n",
    "\n",
    "            nn.Conv1d(in_channels=cnn_start_channels, out_channels=2*cnn_start_channels, kernel_size=30, stride=10),\n",
    "            # output: 256 x 1762\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(2*cnn_start_channels),\n",
    "            # output: 256 x 1762\n",
    "\n",
    "            nn.Conv1d(in_channels=2*cnn_start_channels, out_channels=4*cnn_start_channels, kernel_size=30, stride=10),\n",
    "            # output: 256 x 175\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(4*cnn_start_channels),\n",
    "            # output: 256 x 175\n",
    "        )\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size=4*cnn_start_channels,\n",
    "                            hidden_size=hidden_size, dropout=0.2,\n",
    "                            num_layers=lstm_layers)\n",
    "\n",
    "        #self.rnn = nn.GRU(input_size=4*cnn_start_channels,\n",
    "        #                    hidden_size=hidden_size, dropout=0.2,\n",
    "        #                    num_layers=lstm_layers)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, 10)\n",
    "\n",
    "    def forward(self, inputs, hidden = None):\n",
    "        output = self.conv_layers(inputs)\n",
    "\n",
    "        output = output.transpose(1, 2).transpose(0, 1)\n",
    "\n",
    "        output = torch.tanh(output)\n",
    "        output, hidden = self.rnn(output, hidden)\n",
    "\n",
    "        output = self.fc(output[-1, :, :])\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ExponentialLR, StepLR, CosineAnnealingLR\n",
    "from ignite.contrib.handlers.param_scheduler import LRScheduler\n",
    "\n",
    "model = RNN()\n",
    "\n",
    "run_name = datetime.datetime.now().strftime(\"%Y.%m.%d.%H.%M.%S\")\n",
    "run_name = 'rnn-adam-lr0-001-lr-experiment5'\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=0.1)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "#lr_handler = StepLR(optimizer=optimizer, step_size=200, gamma=0.9)\n",
    "#scheduler = LRScheduler(lr_handler)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "trainer = Trainer_Helper(run_name, device)\n",
    "trainer.setup_dataloader(train_dataloader, val_dataloader, test_dataloader)\n",
    "trainer.add_scheduler(scheduler)\n",
    "trainer.train_rnn(model, optimizer, criterion, 15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "----------\n",
    "\n",
    "Lets run this model on the testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 53.04659498207885\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model = RNN()\n",
    "model = model.to(device)\n",
    "\n",
    "model_filepath = 'saved_models/rnn_adam_best_val=54_8.pth'\n",
    "\n",
    "model.load_state_dict(torch.load(model_filepath))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_dataloader:\n",
    "        imgs = imgs.to(device, dtype=torch.float)\n",
    "        outputs, hidden = model(imgs, None)\n",
    "        _, pred = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (pred.cpu() == labels).sum().item()\n",
    "\n",
    "print('Test Accuracy: {}'.format(100.0*correct/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somewhat surprising this model underperforms heavily compared to the CNN with MFCC spectograms.\n",
    "The paper suggest that this should beat the CNN approach or at least give us a comparable result.\n",
    "We suspect that this could be one of two reasons\n",
    "* This approach works for for sound event detection but is not suited for urban sound classification\n",
    "* We didn't have enough data or should have used data augmentation because the network started overfitting too early"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_venv",
   "language": "python",
   "name": "cuda_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
