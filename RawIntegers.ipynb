{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import csv\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "from utils_train import train, test, fit\n",
    "\n",
    "\n",
    "np.random.seed(123)\n",
    "learning_rate = 0.005\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8732\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "def csvToAudioList(filename,sourceDir):\n",
    "    dataList = []\n",
    "    with open(filename, \"rt\") as csvfile:\n",
    "        lines = csv.reader(csvfile)\n",
    "        dataList = list(lines)\n",
    "        dataList.pop(0)\n",
    "        #delete\n",
    "        #dataList = dataList[1800:2000]\n",
    "        #\n",
    "\n",
    "        audDataset = []\n",
    "        labelDataset = []\n",
    "        print(len(dataList))\n",
    "        for x in dataList:\n",
    "            audData, freq = librosa.load(sourceDir + x[5] +\"/\"+ x[0])\n",
    "            if(len(audData) != 88200):\n",
    "                audData = fillWithZeros(audData)\n",
    "            audDataset.append(audData)\n",
    "            labelDataset.append(labelTrans(x[7]))\n",
    "        print(\"Finished\")\n",
    "\n",
    "    return audDataset, labelDataset\n",
    "\n",
    "def fillWithZeros(audData):\n",
    "    if(len(audData) < 88200):\n",
    "        return np.append(audData,np.zeros((88200-len(audData),1),dtype=np.float32))\n",
    "    else: #One dataset is longer\n",
    "        audData = audData[:88200]\n",
    "        return audData\n",
    "\n",
    "\n",
    "    return audData\n",
    "\n",
    "def labelTrans(labelString):\n",
    "    if(labelString == 'siren'):\n",
    "        return 0\n",
    "    elif(labelString == 'street_music'):\n",
    "        return 1\n",
    "    elif (labelString == 'drilling'):\n",
    "        return 2\n",
    "    elif (labelString == 'dog_bark'):\n",
    "        return 3\n",
    "    elif (labelString == 'children_playing'):\n",
    "        return 4\n",
    "    elif (labelString == 'gun_shot'):\n",
    "        return 5\n",
    "    elif (labelString == 'engine_idling'):\n",
    "        return 6\n",
    "    elif (labelString == 'air_conditioner'):\n",
    "        return 7\n",
    "    elif (labelString == 'jackhammer'):\n",
    "        return 8\n",
    "    elif (labelString == 'car_horn'):\n",
    "        return 9\n",
    "\n",
    "\n",
    "\n",
    "audList,labelList = csvToAudioList('/Users/manueldrazyk/Documents/Uni/FS19/ATML/git/UrbanSound8K.csv','/Users/manueldrazyk/Documents/Uni/FS19/ATML/git/audio/fold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_audio, data_label):\n",
    "\n",
    "        self.data_set = np.array(data_audio)\n",
    "        self.data_label1 = np.array(data_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_set)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_entry = self.data_set[index]\n",
    "        data_entry = torch.from_numpy(data_entry).reshape(4,int(len(self.data_set[index])/4))\n",
    "        data_lab = torch.from_numpy(np.array([self.data_label1[index]]))\n",
    "\n",
    "        return data_entry, data_lab.long()\n",
    "\n",
    "\n",
    "split_refList = int(len(audList)*0.8)\n",
    "split_refListSec = int(len(audList)*0.9)\n",
    "train_audList, val_audList,test_audList = audList[:split_refList], audList[split_refList:split_refListSec], audList[split_refListSec:]\n",
    "train_labelList, val_labelList,test_labelList = labelList[:split_refList], labelList[split_refList:split_refListSec], labelList[split_refListSec:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SimpleConvNet, self).__init__()\n",
    "           \n",
    "        self.conv_layer1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=4, out_channels=16, kernel_size=1, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv_layer2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv_layer3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d((1))\n",
    "        )\n",
    "\n",
    "        self.linear_layer = nn.Sequential(\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.conv_layer1(input)\n",
    "        output = self.conv_layer2(output)\n",
    "        output = self.conv_layer3(output)\n",
    "        \n",
    "        output = output.view(input.size(0), -1)\n",
    "        output = self.linear_layer(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=4, out_channels=16, kernel_size=1, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        )\n",
    "        self.linear_layer = nn.Sequential(\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.conv_layers(input)\n",
    "        output = output.view(input.size(0), -1)\n",
    "        output = self.linear_layer(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = AudioDataset(train_audList,train_labelList)\n",
    "valDataset = AudioDataset(val_audList,val_labelList)\n",
    "testDataset = AudioDataset(test_audList,test_labelList)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(trainDataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_dataloader = torch.utils.data.DataLoader(valDataset, batch_size=32, num_workers=4)\n",
    "test_dataloader = torch.utils.data.DataLoader(testDataset, batch_size=32, num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8: train_loss: 2.7921, train_accuracy: 11.2695, val_loss: 2.2843, val_accuracy: 10.8556\n",
      "Epoch 2/8: train_loss: 2.2722, train_accuracy: 11.8215, val_loss: 2.2717, val_accuracy: 10.8556\n",
      "Epoch 3/8: train_loss: 2.2675, train_accuracy: 11.8215, val_loss: 2.2705, val_accuracy: 10.8556\n",
      "Epoch 4/8: train_loss: 2.2670, train_accuracy: 11.7985, val_loss: 2.2703, val_accuracy: 10.8556\n",
      "Epoch 5/8: train_loss: 2.2669, train_accuracy: 11.7985, val_loss: 2.2703, val_accuracy: 10.8556\n",
      "Epoch 6/8: train_loss: 2.2669, train_accuracy: 11.7985, val_loss: 2.2703, val_accuracy: 10.8556\n",
      "Epoch 7/8: train_loss: 2.2669, train_accuracy: 11.7985, val_loss: 2.2703, val_accuracy: 10.8556\n",
      "Epoch 8/8: train_loss: 2.2669, train_accuracy: 11.7985, val_loss: 2.2703, val_accuracy: 10.8556\n"
     ]
    }
   ],
   "source": [
    "class AudioDatasetRes(Dataset):\n",
    "    def __init__(self, data_audio, data_label):\n",
    "\n",
    "        self.data_set = np.array(data_audio)\n",
    "        self.data_label1 = np.array(data_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_set)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_entry = self.data_set[index]\n",
    "        data_entry = torch.from_numpy(data_entry).reshape(1,4,1,int(len(self.data_set[index])/4))\n",
    "        data_lab = torch.from_numpy(np.array([self.data_label1[index]]))\n",
    "        \n",
    "        return data_entry, data_lab.long()\n",
    "\n",
    "\n",
    "trainDatasetRes = AudioDatasetRes(train_audList,train_labelList)\n",
    "valDatasetRes = AudioDatasetRes(val_audList,val_labelList)\n",
    "\n",
    "epochs = 8\n",
    "RES = models.resnet18()\n",
    "\n",
    "RES.conv1 = nn.Conv2d(4, 64, kernel_size=1, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "optimizerRES = torch.optim.Adam(RES.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizerRES, step_size=10, gamma=0.1)\n",
    "fit(trainDatasetRes,val_dataloader,RES,optimizerRES,loss_fn,epochs, scheduler )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: train_loss: 2.2285, train_accuracy: 15.0752, val_loss: 2.3719, val_accuracy: 12.4857\n",
      "Epoch 2/25: train_loss: 2.1033, train_accuracy: 21.4603, val_loss: 2.2498, val_accuracy: 23.8259\n",
      "Epoch 3/25: train_loss: 1.9090, train_accuracy: 28.8762, val_loss: 2.2042, val_accuracy: 33.5624\n",
      "Epoch 4/25: train_loss: 1.7688, train_accuracy: 34.3737, val_loss: 2.0987, val_accuracy: 29.0951\n",
      "Epoch 5/25: train_loss: 1.6778, train_accuracy: 36.4782, val_loss: 2.1224, val_accuracy: 30.5842\n",
      "Epoch 6/25: train_loss: 1.6154, train_accuracy: 39.7709, val_loss: 2.0290, val_accuracy: 28.0641\n",
      "Epoch 7/25: train_loss: 1.5848, train_accuracy: 41.4030, val_loss: 2.0413, val_accuracy: 32.7606\n",
      "Epoch 8/25: train_loss: 1.5427, train_accuracy: 42.9635, val_loss: 2.0754, val_accuracy: 33.9061\n",
      "Epoch 9/25: train_loss: 1.5192, train_accuracy: 43.8511, val_loss: 1.9952, val_accuracy: 39.7480\n",
      "Epoch 10/25: train_loss: 1.4970, train_accuracy: 45.5548, val_loss: 2.0657, val_accuracy: 38.7171\n",
      "Epoch 11/25: train_loss: 1.4762, train_accuracy: 46.6142, val_loss: 2.1436, val_accuracy: 36.7698\n",
      "Epoch 12/25: train_loss: 1.4165, train_accuracy: 50.3794, val_loss: 2.0656, val_accuracy: 38.6025\n",
      "Epoch 13/25: train_loss: 1.4066, train_accuracy: 50.4223, val_loss: 1.9674, val_accuracy: 42.6117\n",
      "Epoch 14/25: train_loss: 1.4017, train_accuracy: 50.8661, val_loss: 2.0115, val_accuracy: 39.6334\n",
      "Epoch 15/25: train_loss: 1.3969, train_accuracy: 50.7659, val_loss: 1.9960, val_accuracy: 39.8625\n",
      "Epoch 16/25: train_loss: 1.3956, train_accuracy: 51.4674, val_loss: 2.0057, val_accuracy: 41.0080\n"
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "\n",
    "SCN = SimpleConvNet()\n",
    "\n",
    "\n",
    "optimizerSCN = torch.optim.Adam(SCN.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizerSCN, step_size=10, gamma=0.1)\n",
    "fit(train_dataloader,val_dataloader,SCN,optimizerSCN,loss_fn,epochs, scheduler )\n",
    "testLoss, testAcc = test(SCN, test_dataloader, loss_fn)\n",
    "print(\"The accuracy on the testdata is \" + str(testAcc) + \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CN = ConvNet()\n",
    "\n",
    "optimizerCN = torch.optim.Adam(CN.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizerCN, step_size=10, gamma=0.1)\n",
    "fit(train_dataloader,val_dataloader,CN,optimizerCN,loss_fn,epochs, scheduler )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
