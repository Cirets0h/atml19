{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import csv\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "#from utils_train import train, test, fit\n",
    "\n",
    "\n",
    "np.random.seed(123)\n",
    "learning_rate = 0.005\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5435\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "def csvToAudioList(filename,sourceDir):\n",
    "    dataList = []\n",
    "    with open(filename, \"rt\") as csvfile:\n",
    "        lines = csv.reader(csvfile)\n",
    "        dataList = list(lines)\n",
    "        dataList.pop(0)\n",
    "        #delete\n",
    "        #dataList = dataList[1950:2000]\n",
    "        #\n",
    "\n",
    "        audDataset = []\n",
    "        labelDataset = []\n",
    "        print(len(dataList))\n",
    "        for x in dataList:\n",
    "            audData, freq = librosa.load(sourceDir + x[0] + \".wav\")\n",
    "            if(len(audData) != 88200):\n",
    "                audData = fillWithZeros(audData)\n",
    "            audDataset.append(audData)\n",
    "            labelDataset.append(labelTrans(x[1]))\n",
    "        print(\"Finished\")\n",
    "\n",
    "    return audDataset, labelDataset\n",
    "\n",
    "def fillWithZeros(audData):\n",
    "    if(len(audData) < 88200):\n",
    "        return np.append(audData,np.zeros((88200-len(audData),1),dtype=np.float32))\n",
    "    else: #One dataset is longer\n",
    "        audData = audData[:88200]\n",
    "        return audData\n",
    "\n",
    "\n",
    "    return audData\n",
    "\n",
    "def labelTrans(labelString):\n",
    "    if(labelString == 'siren'):\n",
    "        return 0\n",
    "    elif(labelString == 'street_music'):\n",
    "        return 1\n",
    "    elif (labelString == 'drilling'):\n",
    "        return 2\n",
    "    elif (labelString == 'dog_bark'):\n",
    "        return 3\n",
    "    elif (labelString == 'children_playing'):\n",
    "        return 4\n",
    "    elif (labelString == 'gun_shot'):\n",
    "        return 5\n",
    "    elif (labelString == 'engine_idling'):\n",
    "        return 6\n",
    "    elif (labelString == 'air_conditioner'):\n",
    "        return 7\n",
    "    elif (labelString == 'jackhammer'):\n",
    "        return 8\n",
    "    elif (labelString == 'car_horn'):\n",
    "        return 9\n",
    "\n",
    "\n",
    "\n",
    "audList,labelList = csvToAudioList('/Users/manueldrazyk/Documents/Uni/FS19/ATML/Projekt/Proj/Data/urban-sound-classification/train/train.csv','/Users/manueldrazyk/Documents/Uni/FS19/ATML/Projekt/Proj/Data/urban-sound-classification/train/Train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_audio, data_label):\n",
    "\n",
    "        self.data_set = np.array(data_audio)\n",
    "        self.data_label1 = np.array(data_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_set)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_entry = self.data_set[index]\n",
    "        data_entry = torch.from_numpy(data_entry).reshape(1,4,int(len(self.data_set[index])/4))\n",
    "        data_lab = torch.from_numpy(np.array([self.data_label1[index]]))\n",
    "\n",
    "        return data_entry, data_lab.long()\n",
    "\n",
    "\n",
    "split_refList = int(len(audList)*0.8)\n",
    "train_audList, val_audList = audList[:split_refList], audList[split_refList:]\n",
    "train_labelList, val_labelList = labelList[:split_refList], labelList[split_refList:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SimpleConvNet, self).__init__()\n",
    "           \n",
    "        self.conv_layer1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=4, out_channels=16, kernel_size=1, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv_layer2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv_layer3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d((1))\n",
    "        )\n",
    "\n",
    "        self.linear_layer = nn.Sequential(\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.conv_layer1(input)\n",
    "        output = self.conv_layer2(output)\n",
    "        output = self.conv_layer3(output)\n",
    "        \n",
    "        output = output.view(input.size(0), -1)\n",
    "        output = self.linear_layer(output)\n",
    "        return output\n",
    "    \n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def train(model, train_loader, optimizer, loss_fn, print_every=100):\n",
    "    '''\n",
    "    Trains the model for one epoch\n",
    "    '''\n",
    "    n_correct = 0\n",
    "    losses = []\n",
    "    model.to(device=device)\n",
    "    model.train()\n",
    "\n",
    "    for iteration, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device=device)\n",
    "        labels = labels.to(device=device)\n",
    "        output = model(images)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         if iteration % print_every == 0:\n",
    "#             print('Training iteration {}: loss {:.4f}'.format(iteration, loss.item()))\n",
    "        losses.append(loss.item())\n",
    "        n_correct += torch.sum(output.argmax(1) == labels).item()\n",
    "    accuracy = 100.0 * n_correct / len(train_loader.data_set)\n",
    "    return np.mean(np.array(losses)), accuracy\n",
    "            \n",
    "def test(model, test_loader, loss_fn):\n",
    "    '''\n",
    "    Tests the model on data from test_loader\n",
    "    '''\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    n_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            output = model(images)\n",
    "            loss = loss_fn(output, labels)\n",
    "            test_loss += loss.item()\n",
    "            n_correct += torch.sum(output.argmax(1) == labels).item()\n",
    "\n",
    "    average_loss = test_loss / len(test_loader)\n",
    "    accuracy = 100.0 * n_correct / len(test_loader.data_set)\n",
    "#     print('Test average loss: {:.4f}, accuracy: {:.3f}'.format(average_loss, accuracy))\n",
    "    return average_loss, accuracy\n",
    "\n",
    "\n",
    "def fit(train_dataloader, val_dataloader, model, optimizer, loss_fn, n_epochs, scheduler=None):\n",
    "    train_losses, train_accuracies = [], []\n",
    "    val_losses, val_accuracies = [], []\n",
    "    learning_rates = []\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss, train_accuracy = train(model, train_dataloader, optimizer, loss_fn)\n",
    "        val_loss, val_accuracy = test(model, val_dataloader, loss_fn)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        # We'll monitor learning rate -- just to show that it's decreasing\n",
    "        learning_rates.append(optimizer.param_groups[0]['lr'])\n",
    "        ########## Notify a scheduler that an epoch passed\n",
    "        if scheduler:\n",
    "            scheduler.step() # argument only needed for ReduceLROnPlateau\n",
    "        print('Epoch {}/{}: train_loss: {:.4f}, train_accuracy: {:.4f}, val_loss: {:.4f}, val_accuracy: {:.4f}'.format(epoch+1, n_epochs,\n",
    "                                                                                                          train_losses[-1],\n",
    "                                                                                                          train_accuracies[-1],\n",
    "                                                                                                          val_losses[-1],\n",
    "                                                                                                          val_accuracies[-1]))\n",
    "\n",
    "    print(learning_rates)\n",
    "    return train_losses, train_accuracies, val_losses, val_accuracies, learning_rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: train_loss: 2.2674, train_accuracy: 12.2585, val_loss: 2.2352, val_accuracy: 16.3753\n",
      "Epoch 2/25: train_loss: 2.1821, train_accuracy: 18.0543, val_loss: 2.0808, val_accuracy: 22.1711\n",
      "Epoch 3/25: train_loss: 2.0145, train_accuracy: 27.0699, val_loss: 1.9696, val_accuracy: 28.5189\n",
      "Epoch 4/25: train_loss: 1.8820, train_accuracy: 31.8537, val_loss: 1.8205, val_accuracy: 33.9466\n",
      "Epoch 5/25: train_loss: 1.7774, train_accuracy: 36.1086, val_loss: 1.7430, val_accuracy: 36.7065\n",
      "Epoch 6/25: train_loss: 1.7133, train_accuracy: 38.7764, val_loss: 1.6924, val_accuracy: 37.9945\n",
      "Epoch 7/25: train_loss: 1.6748, train_accuracy: 39.8574, val_loss: 1.6581, val_accuracy: 38.8224\n",
      "Epoch 8/25: train_loss: 1.6422, train_accuracy: 40.8694, val_loss: 1.6471, val_accuracy: 40.0184\n",
      "Epoch 9/25: train_loss: 1.6201, train_accuracy: 41.9043, val_loss: 1.6373, val_accuracy: 40.6624\n",
      "Epoch 10/25: train_loss: 1.5997, train_accuracy: 42.8473, val_loss: 1.6310, val_accuracy: 41.3983\n",
      "Epoch 11/25: train_loss: 1.5821, train_accuracy: 43.3073, val_loss: 1.6278, val_accuracy: 41.2144\n",
      "Epoch 12/25: train_loss: 1.5019, train_accuracy: 46.5501, val_loss: 1.5700, val_accuracy: 42.3183\n",
      "Epoch 13/25: train_loss: 1.4874, train_accuracy: 47.3781, val_loss: 1.5580, val_accuracy: 42.3183\n",
      "Epoch 14/25: train_loss: 1.4809, train_accuracy: 47.5391, val_loss: 1.5498, val_accuracy: 42.7783\n",
      "Epoch 15/25: train_loss: 1.4761, train_accuracy: 47.9301, val_loss: 1.5439, val_accuracy: 43.3303\n",
      "Epoch 16/25: train_loss: 1.4723, train_accuracy: 48.0451, val_loss: 1.5395, val_accuracy: 42.9623\n",
      "Epoch 17/25: train_loss: 1.4686, train_accuracy: 48.0911, val_loss: 1.5357, val_accuracy: 42.9623\n",
      "Epoch 18/25: train_loss: 1.4652, train_accuracy: 48.5051, val_loss: 1.5327, val_accuracy: 42.7783\n",
      "Epoch 19/25: train_loss: 1.4612, train_accuracy: 48.2981, val_loss: 1.5289, val_accuracy: 43.4223\n",
      "Epoch 20/25: train_loss: 1.4580, train_accuracy: 48.2981, val_loss: 1.5259, val_accuracy: 42.8703\n",
      "Epoch 21/25: train_loss: 1.4545, train_accuracy: 48.1141, val_loss: 1.5226, val_accuracy: 43.7902\n",
      "Epoch 22/25: train_loss: 1.4442, train_accuracy: 48.8040, val_loss: 1.5152, val_accuracy: 44.6182\n",
      "Epoch 23/25: train_loss: 1.4414, train_accuracy: 48.6661, val_loss: 1.5131, val_accuracy: 44.3422\n",
      "Epoch 24/25: train_loss: 1.4403, train_accuracy: 48.7351, val_loss: 1.5122, val_accuracy: 44.5262\n",
      "Epoch 25/25: train_loss: 1.4400, train_accuracy: 48.6431, val_loss: 1.5116, val_accuracy: 44.6182\n",
      "[0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 5.000000000000001e-05, 5.000000000000001e-05, 5.000000000000001e-05, 5.000000000000001e-05]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([2.267382847352541,\n",
       "  2.1820643112652873,\n",
       "  2.014481748417431,\n",
       "  1.8819826933441353,\n",
       "  1.7773694755799088,\n",
       "  1.7132539797159598,\n",
       "  1.6747914576541423,\n",
       "  1.6422185016754867,\n",
       "  1.6201305266760166,\n",
       "  1.5996602061971432,\n",
       "  1.5821348454823791,\n",
       "  1.501882975351703,\n",
       "  1.4874296242357492,\n",
       "  1.4809446119101035,\n",
       "  1.476146111244262,\n",
       "  1.4723094263702645,\n",
       "  1.4685845589445152,\n",
       "  1.4651872829980497,\n",
       "  1.4612376250535688,\n",
       "  1.4580259144986467,\n",
       "  1.4545194743147158,\n",
       "  1.444172652521391,\n",
       "  1.44141231177044,\n",
       "  1.4402613594867597,\n",
       "  1.4399861360642212],\n",
       " [12.258509659613615,\n",
       "  18.054277828886846,\n",
       "  27.069917203311867,\n",
       "  31.85372585096596,\n",
       "  36.10855565777369,\n",
       "  38.77644894204232,\n",
       "  39.857405703771846,\n",
       "  40.869365225390986,\n",
       "  41.904323827046916,\n",
       "  42.84728610855566,\n",
       "  43.30726770929163,\n",
       "  46.550137994480224,\n",
       "  47.378104875804965,\n",
       "  47.53909843606256,\n",
       "  47.93008279668813,\n",
       "  48.045078196872126,\n",
       "  48.09107635694572,\n",
       "  48.5050597976081,\n",
       "  48.29806807727691,\n",
       "  48.29806807727691,\n",
       "  48.11407543698252,\n",
       "  48.804047838086476,\n",
       "  48.666053357865685,\n",
       "  48.735050597976084,\n",
       "  48.64305427782889],\n",
       " [2.235217796462111,\n",
       "  2.080791286706047,\n",
       "  1.9696336412440776,\n",
       "  1.8205174495703382,\n",
       "  1.7430265196003603,\n",
       "  1.6923908352056773,\n",
       "  1.6580890423679089,\n",
       "  1.6470983908978645,\n",
       "  1.63733619139654,\n",
       "  1.630976455164482,\n",
       "  1.6278034395297514,\n",
       "  1.56999952860156,\n",
       "  1.5579565682730327,\n",
       "  1.549809619304557,\n",
       "  1.5438839505394886,\n",
       "  1.5394923479022953,\n",
       "  1.5356939662018279,\n",
       "  1.5326755349665688,\n",
       "  1.5288513622074469,\n",
       "  1.5259303955469201,\n",
       "  1.5226410963086765,\n",
       "  1.5151664884461102,\n",
       "  1.51314292331299,\n",
       "  1.5122250905114791,\n",
       "  1.5116071663625545],\n",
       " [16.375344986200552,\n",
       "  22.17111315547378,\n",
       "  28.518859245630175,\n",
       "  33.94664213431463,\n",
       "  36.70653173873045,\n",
       "  37.99448022079117,\n",
       "  38.822447102115916,\n",
       "  40.01839926402944,\n",
       "  40.6623735050598,\n",
       "  41.39834406623735,\n",
       "  41.214351425942965,\n",
       "  42.318307267709294,\n",
       "  42.318307267709294,\n",
       "  42.77828886844526,\n",
       "  43.33026678932843,\n",
       "  42.96228150873965,\n",
       "  42.96228150873965,\n",
       "  42.77828886844526,\n",
       "  43.42226310947562,\n",
       "  42.870285188592455,\n",
       "  43.7902483900644,\n",
       "  44.61821527138915,\n",
       "  44.34222631094756,\n",
       "  44.52621895124195,\n",
       "  44.61821527138915],\n",
       " [0.005,\n",
       "  0.005,\n",
       "  0.005,\n",
       "  0.005,\n",
       "  0.005,\n",
       "  0.005,\n",
       "  0.005,\n",
       "  0.005,\n",
       "  0.005,\n",
       "  0.005,\n",
       "  0.005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  5.000000000000001e-05,\n",
       "  5.000000000000001e-05,\n",
       "  5.000000000000001e-05,\n",
       "  5.000000000000001e-05])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataset = AudioDataset(train_audList,train_labelList)\n",
    "valDataset = AudioDataset(val_audList,val_labelList)\n",
    "\n",
    "\n",
    "model = SimpleConvNet()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "epochs = 25\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=10, gamma=0.1)\n",
    "fit(trainDataset,valDataset,model,optimizer,loss_fn,epochs, scheduler )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
